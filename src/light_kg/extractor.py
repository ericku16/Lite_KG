import os
import json
import time
from typing import List

from .core.clients import get_llm_client, BaseLLMClient
from .core.models import Triple, EntityDetail
from .steps import OntologyFilter, NERLinker, RelationExtractor
from .utils.file_loader import load_document

class Extractor:
    """
    ä¸»è¦çš„çŸ¥è­˜åœ–è­œæå–å™¨ 
    é€™å€‹é¡åˆ¥åˆå§‹åŒ–ä¸¦å”èª¿æ‰€æœ‰æ­¥é©Ÿ
    """

    def __init__(self, 
        provider: str,
        model_name: str, 
        ner_model_path: str, 
        api_key: str = None):

        """
        Args:
            provider (str): "openai" æˆ– "ollama"
            model_name (str): æ¨¡å‹çš„åç¨± (ä¾‹å¦‚ "gpt-4o", "mistral:latest")
            ner_model_path (str): æœ¬åœ° Flair NER æ¨¡å‹çš„è·¯å¾‘
            api_key (str, optional): OpenAI API é‡‘é‘°
        """
        print(f"--- æ­£åœ¨åˆå§‹åŒ– Extractor ---")
        print(f"  Provider: {provider}")
        print(f"  LLM Model: {model_name}")
        print(f"  NER Model: {ner_model_path}")
        
        # 1. åˆå§‹åŒ– LLM å®¢æˆ¶ç«¯ (ç”¨æ–¼ Step 1 å’Œ Step 3)
        self.llm_client = get_llm_client(provider, model_name, api_key)
        
        # 2. åˆå§‹åŒ–æ‰€æœ‰æ­¥é©Ÿ
        self.filterer = OntologyFilter(self.llm_client)
        self.ner_linker = NERLinker(ner_model_path)
        self.extractor = RelationExtractor(self.llm_client)
        print("--- åˆå§‹åŒ–å®Œæˆ ---\n")

    def process_documents(
        self, 
        folder_path: str, 
        output_json_path: str, 
        chunk_size: int = 5000, 
        delay_between_chunks: int = 1
    ) -> List[Triple]:
        """
        è™•ç†è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æ–‡ä»¶ä¸¦æå–é—œä¿‚ä¸‰å…ƒçµ„
        """
        
        print(f"\n--- é–‹å§‹è™•ç†è³‡æ–™å¤¾ '{folder_path}' ---")
        output_dir = os.path.dirname(output_json_path)
        if not os.path.exists(output_dir): 
            os.makedirs(output_dir)
            
        all_extracted_triples: List[Triple] = []
        start_time = time.time()

        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            if not (os.path.isfile(file_path) and (filename.endswith(".txt") or filename.endswith(".pdf"))):
                continue
                
            print(f"\nğŸ“„ æ­£åœ¨è™•ç†æª”æ¡ˆ: {filename}")
            
            # ä½¿ç”¨å·¥å…·å‡½å¼è®€å–æª”æ¡ˆ
            content = load_document(file_path)
            if not content:
                print(f"  - æª”æ¡ˆå…§å®¹ç‚ºç©º è·³é {filename}ã€‚")
                continue

            # --- æµç¨‹é–‹å§‹ ---
            try:
                # Step 1: Ontology Filtering
                filtered_content = self.filterer.filter_text(content)
                if not filtered_content or not filtered_content.strip():
                    print("  - éæ¿¾å¾Œç„¡ç›¸é—œå…§å®¹ è·³éæ­¤æª”æ¡ˆ")
                    continue

                # åˆ‡åˆ†æ–‡æœ¬
                text_chunks = [filtered_content[i:i + chunk_size] for i in range(0, len(filtered_content), chunk_size)]
                
                for i, chunk in enumerate(text_chunks):
                    print(f"\n  â†’ æ­£åœ¨è™•ç†å€å¡Š {i+1}/{len(text_chunks)}...")

                    # Step 2 & 2.5: NER + Entity Linking
                    # ner_link_result åŒ…å« .lookup_map å’Œ .canonical_entities_for_re
                    ner_link_result = self.ner_linker.link_entities(chunk)
                    
                    if not ner_link_result.canonical_entities_for_re:
                        print("  - Step 2.5 (Linking): æœªèƒ½æ‰¾åˆ°è¶³å¤ çš„å¯¦é«” è·³éæ­¤å€å¡Š")
                        continue

                    # Step 3: LLM-RE (NRE)
                    # ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„å¯¦é«”æ¸…å–®é€²è¡Œé—œä¿‚æå–
                    relations = self.extractor.extract_relations(
                        chunk, 
                        ner_link_result.canonical_entities_for_re
                    )
                    
                    if relations:
                        print(f"  - æ­£åœ¨é€²è¡Œæœ€çµ‚æ ¼å¼åŒ–...")
                        formatted_count = 0
                        for rel in relations:
                            if isinstance(rel, list) and len(rel) == 3:
                                s, p, o = rel
                                
                                # æ ¹æ“š RE å›å‚³çš„æ¨™æº–åŒ–åç¨±ï¼Œå¾ lookup map ä¸­æŸ¥æ‰¾ Wikidata ID
                                s_info = ner_link_result.lookup_map.get(s.lower())
                                o_info = ner_link_result.lookup_map.get(o.lower())
                                
                                s_canonical = s_info['canonical_name'] if s_info else s
                                s_id = s_info['wikidata_id'] if s_info else None
                                
                                o_canonical = o_info['canonical_name'] if o_info else o
                                o_id = o_info['wikidata_id'] if o_info else None
                                
                                # ä½¿ç”¨ Pydantic æ¨¡å‹
                                final_triple = Triple(
                                    subject=EntityDetail(name=s, canonical_name=s_canonical, wikidata_id=s_id),
                                    predicate=p,
                                    object=EntityDetail(name=o, canonical_name=o_canonical, wikidata_id=o_id)
                                )
                                all_extracted_triples.append(final_triple)
                                formatted_count += 1

                        print(f"  âœ… æˆåŠŸè™•ç† {formatted_count} ç­†é—œè¯ä¸¦åŠ å…¥ Wikidata ID ")

                    if i < len(text_chunks) - 1:
                        time.sleep(delay_between_chunks)
                        
            except Exception as e:
                print(f"  âŒ è™•ç†æª”æ¡ˆ {filename} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}")

        # --- å„²å­˜çµæœ ---
        print(f"\n--- æ­£åœ¨å°‡çµæœå¯«å…¥ '{output_json_path}'... ---")
        
        # å°‡ Pydantic æ¨¡å‹è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨ä»¥ä¾¿ JSON å„²å­˜
        results_data = [triple.model_dump() for triple in all_extracted_triples]
        
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸš€ JSON æª”æ¡ˆç”¢å‡ºæˆåŠŸï¼å…±æå– {len(all_extracted_triples)} ç­†é—œè¯")

        # --- é¡¯ç¤ºç¸½è€—æ™‚ ---
        end_time = time.time()
        elapsed_time = end_time - start_time
        minutes = int(elapsed_time // 60)
        seconds = int(elapsed_time % 60)
        print("\n" + "="*50)
        print(f"â±ï¸  ç¸½è€—æ™‚: {minutes} åˆ† {seconds} ç§’ ({elapsed_time:.2f} ç§’)")
        
        # é¡¯ç¤ºå¯¦é«”é€£çµç¸½è€—æ™‚
        linking_minutes = int(self.ner_linker.total_linking_time // 60)
        linking_seconds = int(self.ner_linker.total_linking_time % 60)
        print(f"ğŸ”— å¯¦é«”é€£çµ (API æŸ¥è©¢) ç¸½è€—æ™‚: {linking_minutes} åˆ† {linking_seconds} ç§’ ({self.ner_linker.total_linking_time:.2f} ç§’)")
        print("="*50)

        return all_extracted_triples